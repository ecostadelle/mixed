{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tableshift import get_dataset\n",
    "\n",
    "dset = get_dataset(\n",
    "    name='college_scorecard', \n",
    "    cache_dir='../tableshift/tmp', \n",
    "    use_cached=True\n",
    "    )\n",
    "\n",
    "X_a, y_a, _, _ = dset.get_pandas('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5.,  3.,  2.,  6., -1.,  1.,  9.,  7.,  4.,  8.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a.PCIP03.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('../tableshift/tmp/kaggle/college-scorecard/Scorecard.csv', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fslab/github/mixed/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550: DtypeWarning: Columns (2,7,8,9,10,20,24,25,26,27,28,29,30,31,32,33,34,35,36,100,101,102,290,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "scorecard = pd.read_csv('../tableshift/tmp/kaggle/college-scorecard/Scorecard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124699,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorecard.PCIP03.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1,100636,01230800,012308,COMMUNITY COLLEGE OF THE AIR FORCE,MONTGOMERY,AL,361126613,,,,,0,Main campus,1,Predominantly associate's-degree granting,Associate degree,Public,Alabama,U.S. Service Schools,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0.0029,0.0367,0.0122,0,0.1247,0,0.159,0,0.0011,0.0032,0,0,0,0,0,0.0463,0,0.0035,0,0,0.0084,0.0022,0,0.0644,0.0046,0,0,0.1984,0,0.0478,0.0048,0.0803,0.1994,0.0002,Program not offered,Program not offered,Program not offered,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,,44141,,,,,,,,,0,0,0.7966,0.1536,0.0163,0.0033,0.0302,,,,,,,,0.9752,,Currently certified as operating,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1996\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} & Unnamed: 1 &  CatBoost &  LightGBM &   NODE &  ResNet &  ModAdapBRS \\\\\n",
      "Estimator              &            &           &           &        &         &             \\\\\n",
      "\\midrule\n",
      "ASSISTments            &         ID &     0.943 &     0.936 &  0.935 &   0.933 &         NaN \\\\\n",
      "ASSISTments            &        OOD &     0.584 &     0.591 &  0.583 &   0.583 &         NaN \\\\\n",
      "Childhood Lead         &         ID &     0.971 &     0.971 &  0.971 &   0.971 &         NaN \\\\\n",
      "Childhood Lead         &        OOD &     0.920 &     0.920 &  0.920 &   0.920 &         NaN \\\\\n",
      "College Scorecard      &         ID &     0.957 &     0.939 &  0.944 &   0.947 &         NaN \\\\\n",
      "College Scorecard      &        OOD &     0.885 &     0.822 &  0.844 &   0.854 &         NaN \\\\\n",
      "Diabetes               &         ID &     0.877 &     0.876 &  0.877 &   0.874 &         NaN \\\\\n",
      "Diabetes               &        OOD &     0.833 &     0.833 &  0.833 &   0.829 &         NaN \\\\\n",
      "FICO HELOC             &         ID &     0.727 &     0.647 &  0.745 &   0.748 &         NaN \\\\\n",
      "FICO HELOC             &        OOD &     0.582 &     0.421 &  0.431 &   0.431 &         NaN \\\\\n",
      "Food Stamps            &         ID &     0.849 &     0.836 &  0.849 &   0.843 &         NaN \\\\\n",
      "Food Stamps            &        OOD &     0.825 &     0.808 &  0.822 &   0.820 &         NaN \\\\\n",
      "Hospital Readmission   &         ID &     0.659 &     0.658 &  0.659 &   0.639 &         NaN \\\\\n",
      "Hospital Readmission   &        OOD &     0.618 &     0.598 &  0.624 &   0.581 &         NaN \\\\\n",
      "Hypertension           &         ID &     0.670 &     0.678 &  0.670 &   0.667 &         NaN \\\\\n",
      "Hypertension           &        OOD &     0.599 &     0.634 &  0.597 &   0.608 &         NaN \\\\\n",
      "ICU Hospital Mortality &         ID &     0.934 &     0.946 &  0.915 &   0.915 &         NaN \\\\\n",
      "ICU Hospital Mortality &        OOD &     0.892 &     0.883 &  0.876 &   0.876 &         NaN \\\\\n",
      "ICU Length of Stay     &         ID &     0.710 &     0.689 &  0.661 &   0.606 &         NaN \\\\\n",
      "ICU Length of Stay     &        OOD &     0.674 &     0.655 &  0.609 &   0.577 &         NaN \\\\\n",
      "Income                 &         ID &     0.832 &     0.822 &  0.831 &   0.826 &         NaN \\\\\n",
      "Income                 &        OOD &     0.814 &     0.809 &  0.810 &   0.815 &         NaN \\\\\n",
      "Public Health Ins      &         ID &     0.814 &     0.803 &  0.811 &   0.810 &         NaN \\\\\n",
      "Public Health Ins.     &        OOD &     0.690 &     0.639 &  0.662 &   0.672 &         NaN \\\\\n",
      "Sepsis                 &         ID &     0.988 &     0.988 &  0.988 &   0.988 &         NaN \\\\\n",
      "Sepsis                 &        OOD &     0.925 &     0.928 &  0.925 &   0.925 &         NaN \\\\\n",
      "Unemployment           &         ID &     0.973 &     0.973 &  0.973 &   0.972 &         NaN \\\\\n",
      "Unemployment           &        OOD &     0.962 &     0.960 &  0.962 &   0.959 &         NaN \\\\\n",
      "Voting                 &         ID &     0.883 &     0.881 &  0.885 &   0.887 &         NaN \\\\\n",
      "Voting                 &        OOD &     0.855 &     0.855 &  0.851 &   0.836 &         NaN \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_clipboard(sep='\\t', decimal=',',index_col=0).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIL\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.  $\\diamondsuit$: the large number of training subdomains (over $700$ for ASSISTments) makes training domain generalization models impractical. $\\square$: the large dataset size makes training adversarial label DRO models impractical (since per-example gradients must be computed). We leave these experiments to future work.}\n",
    "\\label{tab:full-results-assistments-lead}\n",
    "\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{ASSISTments}} & \\multicolumn{4}{c}{\\textbf{Childhood Lead}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & $\\square$ & $\\square$ & $\\square$ & $\\square$ & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tCatBoost & 0.943 & (0.942, 0.944) & 0.584 & (0.562, 0.607) & 0.971 & (0.961, 0.979) & 0.92 & (0.914, 0.925) \\\\\n",
    "\tDRO & 0.932 & (0.931, 0.933) & 0.583 & (0.561, 0.606) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tFT-Transformer & 0.939 & (0.938, 0.94) & 0.592 & (0.569, 0.614) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tLabel Group DRO & 0.928 & (0.927, 0.929) & 0.574 & (0.551, 0.596) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tLightGBM & 0.936 & (0.935, 0.937) & 0.591 & (0.568, 0.613) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tMLP & 0.933 & (0.932, 0.934) & 0.583 & (0.561, 0.606) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tNODE & 0.935 & (0.934, 0.936) & 0.583 & (0.561, 0.606) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tResNet & 0.933 & (0.932, 0.934) & 0.583 & (0.561, 0.606) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tSAINT & 0.935 & (0.934, 0.936) & 0.584 & (0.562, 0.607) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tTabTransformer & 0.93 & (0.929, 0.93) & 0.551 & (0.529, 0.574) & 0.971 & (0.961, 0.979) & 0.92 & (0.915, 0.925) \\\\\n",
    "\tXGBoost & 0.93 & (0.929, 0.931) & 0.591 & (0.568, 0.613) & 0.971 & (0.961, 0.979) & 0.92 & (0.914, 0.925) \\\\\n",
    "\tCORAL & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tDANN & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tGroup DRO & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tIRM & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMMD & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMixUp& $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tVREX & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\diamondsuit$ & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-collegescorecard-diabetes}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{College Scorecard}} & \\multicolumn{4}{c}{\\textbf{Diabetes}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.937 & (0.933, 0.942) & 0.826 & (0.805, 0.846) & 0.877 & (0.875, 0.878) & 0.832 & (0.83, 0.833) \\\\\n",
    "\tCatBoost & 0.957 & (0.954, 0.961) & 0.885 & (0.866, 0.901) & 0.877 & (0.876, 0.879) & 0.833 & (0.831, 0.835) \\\\\n",
    "\tDRO & 0.95 & (0.946, 0.954) & 0.862 & (0.842, 0.88) & 0.876 & (0.875, 0.878) & 0.832 & (0.83, 0.834) \\\\\n",
    "\tFT-Transformer & 0.948 & (0.944, 0.952) & 0.859 & (0.839, 0.877) & 0.877 & (0.875, 0.879) & 0.832 & (0.831, 0.834) \\\\\n",
    "\tLabel Group DRO & 0.928 & (0.924, 0.933) & 0.817 & (0.796, 0.838) & 0.876 & (0.874, 0.878) & 0.831 & (0.83, 0.833) \\\\\n",
    "\tLightGBM & 0.939 & (0.935, 0.943) & 0.822 & (0.8, 0.841) & 0.876 & (0.874, 0.878) & 0.833 & (0.831, 0.835) \\\\\n",
    "\tMLP & 0.947 & (0.942, 0.95) & 0.845 & (0.825, 0.864) & 0.877 & (0.875, 0.879) & 0.832 & (0.83, 0.833) \\\\\n",
    "\tNODE & 0.944 & (0.939, 0.948) & 0.844 & (0.823, 0.863) & 0.877 & (0.875, 0.879) & 0.833 & (0.832, 0.835) \\\\\n",
    "\tResNet & 0.947 & (0.943, 0.951) & 0.854 & (0.834, 0.872) & 0.874 & (0.872, 0.876) & 0.829 & (0.828, 0.831) \\\\\n",
    "\tSAINT & 0.936 & (0.931, 0.94) & 0.814 & (0.792, 0.834) & 0.877 & (0.875, 0.879) & 0.833 & (0.831, 0.834) \\\\\n",
    "\tTabTransformer & 0.942 & (0.938, 0.946) & 0.845 & (0.825, 0.864) & 0.875 & (0.873, 0.877) & 0.83 & (0.829, 0.832) \\\\\n",
    "\tXGBoost & 0.942 & (0.938, 0.946) & 0.83 & (0.809, 0.85) & 0.877 & (0.875, 0.879) & 0.832 & (0.83, 0.834) \\\\\n",
    "\tCORAL & 0.922 & (0.917, 0.926) & 0.795 & (0.773, 0.816) & 0.874 & (0.872, 0.875) & 0.832 & (0.83, 0.834) \\\\\n",
    "\tDANN & 0.894 & (0.889, 0.9) & 0.78 & (0.757, 0.802) & 0.873 & (0.871, 0.875) & 0.826 & (0.824, 0.827) \\\\\n",
    "\tGroup DRO & 0.944 & (0.939, 0.948) & 0.829 & (0.808, 0.849) & 0.877 & (0.875, 0.879) & 0.832 & (0.83, 0.833) \\\\\n",
    "\tIRM & 0.879 & (0.873, 0.885) & 0.746 & (0.721, 0.769) & 0.873 & (0.871, 0.875) & 0.826 & (0.824, 0.827) \\\\\n",
    "\tMMD & 0.925 & (0.92, 0.929) & 0.795 & (0.773, 0.816) & 0.873 & (0.871, 0.875) & 0.826 & (0.825, 0.828) \\\\\n",
    "\tMixUp & 0.912 & (0.907, 0.917) & 0.746 & (0.721, 0.769) & 0.873 & (0.871, 0.875) & 0.826 & (0.824, 0.827) \\\\\n",
    "\tVREX & 0.907 & (0.902, 0.912) & 0.754 & (0.731, 0.777) & 0.873 & (0.871, 0.875) & 0.826 & (0.824, 0.827) \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-heloc-foodstamps}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{FICO HELOC}} & \\multicolumn{4}{c}{\\textbf{Food Stamps}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.843 & (0.84, 0.846) & 0.812 & (0.808, 0.815) \\\\\n",
    "\tCatBoost & 0.727 & (0.67, 0.778) & 0.582 & (0.57, 0.594) & 0.849 & (0.847, 0.852) & 0.825 & (0.821, 0.828) \\\\\n",
    "\tDRO & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.844 & (0.841, 0.846) & 0.819 & (0.815, 0.822) \\\\\n",
    "\tFT-Transformer & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.843 & (0.841, 0.846) & 0.816 & (0.812, 0.819) \\\\\n",
    "\tLabel Group DRO & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.771 & (0.768, 0.774) & 0.752 & (0.748, 0.756) \\\\\n",
    "\tLightGBM & 0.647 & (0.584, 0.7) & 0.421 & (0.409, 0.433) & 0.836 & (0.833, 0.838) & 0.808 & (0.805, 0.812) \\\\\n",
    "\tMLP & 0.734 & (0.678, 0.785) & 0.538 & (0.526, 0.55) & 0.841 & (0.838, 0.844) & 0.815 & (0.812, 0.819) \\\\\n",
    "\tNODE & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.849 & (0.847, 0.852) & 0.822 & (0.819, 0.825) \\\\\n",
    "\tResNet & 0.748 & (0.693, 0.798) & 0.431 & (0.42, 0.443) & 0.843 & (0.84, 0.845) & 0.82 & (0.817, 0.824) \\\\\n",
    "\tSAINT & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.849 & (0.846, 0.851) & 0.821 & (0.818, 0.825) \\\\\n",
    "\tTabTransformer & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.836 & (0.834, 0.839) & 0.807 & (0.803, 0.81) \\\\\n",
    "\tXGBoost & 0.745 & (0.689, 0.795) & 0.431 & (0.419, 0.443) & 0.844 & (0.842, 0.847) & 0.82 & (0.817, 0.824) \\\\\n",
    "\tCORAL & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.818 & (0.815, 0.82) & 0.793 & (0.79, 0.797) \\\\\n",
    "\tDANN & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.809 & (0.806, 0.812) & 0.78 & (0.776, 0.784) \\\\\n",
    "\tGroup DRO & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.84 & (0.838, 0.843) & 0.817 & (0.814, 0.821) \\\\\n",
    "\tIRM & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.812 & (0.81, 0.815) & 0.795 & (0.791, 0.798) \\\\\n",
    "\tMMD & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.813 & (0.81, 0.816) & 0.786 & (0.782, 0.789) \\\\\n",
    "\tMixUp & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.819 & (0.816, 0.821) & 0.785 & (0.782, 0.789) \\\\\n",
    "\tVREX & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.809 & (0.806, 0.812) & 0.78 & (0.776, 0.784) \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-readmission-hypertension}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{Hospital Readmission}} & \\multicolumn{4}{c}{\\textbf{Hypertension}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.655 & (0.641, 0.669) & 0.603 & (0.599, 0.607) & 0.666 & (0.66, 0.672) & 0.601 & (0.6, 0.603) \\\\\n",
    "\tCatBoost & 0.659 & (0.645, 0.674) & 0.618 & (0.614, 0.623) & 0.67 & (0.665, 0.676) & 0.599 & (0.597, 0.6) \\\\\n",
    "\tDRO & 0.628 & (0.613, 0.642) & 0.578 & (0.574, 0.582) & 0.598 & (0.592, 0.604) & 0.416 & (0.414, 0.417) \\\\\n",
    "\tFT-Transformer & 0.648 & (0.633, 0.662) & 0.618 & (0.614, 0.622) & 0.666 & (0.661, 0.672) & 0.604 & (0.603, 0.605) \\\\\n",
    "\tLabel Group DRO & 0.652 & (0.637, 0.666) & 0.616 & (0.612, 0.62) & 0.665 & (0.659, 0.671) & 0.604 & (0.603, 0.605) \\\\\n",
    "\tLightGBM & 0.658 & (0.643, 0.672) & 0.598 & (0.594, 0.602) & 0.678 & (0.672, 0.683) & 0.634 & (0.633, 0.635) \\\\\n",
    "\tMLP & 0.648 & (0.633, 0.662) & 0.612 & (0.608, 0.617) & 0.664 & (0.658, 0.67) & 0.583 & (0.582, 0.584) \\\\\n",
    "\tNODE & 0.659 & (0.645, 0.673) & 0.624 & (0.62, 0.628) & 0.67 & (0.664, 0.676) & 0.597 & (0.596, 0.599) \\\\\n",
    "\tResNet & 0.639 & (0.624, 0.653) & 0.581 & (0.577, 0.586) & 0.667 & (0.661, 0.672) & 0.608 & (0.606, 0.609) \\\\\n",
    "\tSAINT & 0.654 & (0.639, 0.668) & 0.61 & (0.606, 0.615) & 0.669 & (0.664, 0.675) & 0.595 & (0.594, 0.596) \\\\\n",
    "\tTabTransformer & 0.584 & (0.569, 0.599) & 0.507 & (0.502, 0.511) & 0.624 & (0.618, 0.63) & 0.499 & (0.498, 0.501) \\\\\n",
    "\tXGBoost & 0.651 & (0.636, 0.665) & 0.605 & (0.601, 0.61) & 0.671 & (0.665, 0.677) & 0.588 & (0.587, 0.59) \\\\\n",
    "\tCORAL & 0.622 & (0.607, 0.637) & 0.571 & (0.567, 0.576) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tDANN & 0.584 & (0.569, 0.599) & 0.506 & (0.502, 0.51) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tGroup DRO & 0.639 & (0.624, 0.653) & 0.6 & (0.596, 0.605) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tIRM & 0.595 & (0.58, 0.61) & 0.55 & (0.546, 0.555) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMMD & 0.626 & (0.611, 0.64) & 0.57 & (0.565, 0.574) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMixUp & 0.589 & (0.574, 0.604) & 0.567 & (0.563, 0.572) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tVREX & 0.584 & (0.569, 0.599) & 0.506 & (0.502, 0.51) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "    \n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}. $\\heartsuit$: the large feature dimensionality of both ICU datasets makes training Transformer-based models impractical (e.g. even a single-layer SAINT model requires $>$13B trainable parameters on both ICU datasets)}\n",
    "\\label{tab:full-results-icu-both}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{ICU Hospital Mortality}} & \\multicolumn{4}{c}{\\textbf{ICU Length of Stay}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.602 & (0.572, 0.631) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tCatBoost & 0.934 & (0.914, 0.948) & 0.892 & (0.887, 0.897) & 0.71 & (0.682, 0.737) & 0.674 & (0.665, 0.682) \\\\\n",
    "\tDRO & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.601 & (0.571, 0.63) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tFT-Transformer & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ \\\\\n",
    "\tLabel Group DRO & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.59 & (0.56, 0.619) & 0.542 & (0.533, 0.551) \\\\\n",
    "\tLightGBM & 0.946 & (0.928, 0.959) & 0.883 & (0.877, 0.888) & 0.689 & (0.66, 0.716) & 0.655 & (0.646, 0.663) \\\\\n",
    "\tMLP & 0.912 & (0.891, 0.929) & 0.877 & (0.871, 0.882) & 0.599 & (0.569, 0.628) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tNODE & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.661 & (0.632, 0.689) & 0.609 & (0.6, 0.618) \\\\\n",
    "\tResNet & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.606 & (0.576, 0.635) & 0.577 & (0.568, 0.586) \\\\\n",
    "\tSAINT & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ & $\\heartsuit$ \\\\\n",
    "\tTabTransformer & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.604 & (0.574, 0.633) & 0.549 & (0.54, 0.558) \\\\\n",
    "\tXGBoost & 0.927 & (0.908, 0.943) & 0.882 & (0.876, 0.887) & 0.71 & (0.682, 0.737) & 0.669 & (0.66, 0.677) \\\\\n",
    "\tCORAL & 0.915 & (0.893, 0.931) & 0.875 & (0.869, 0.881) & 0.603 & (0.573, 0.632) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tDANN & 0.915 & (0.893, 0.931) & 0.876 & (0.871, 0.882) & 0.594 & (0.564, 0.624) & 0.545 & (0.536, 0.554) \\\\\n",
    "\tGroup DRO & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.602 & (0.572, 0.631) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tIRM & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.601 & (0.571, 0.63) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tMMD & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.602 & (0.572, 0.631) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tMixUp & 0.915 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.602 & (0.572, 0.631) & 0.544 & (0.535, 0.553) \\\\\n",
    "\tVREX & 0.913 & (0.893, 0.931) & 0.876 & (0.87, 0.882) & 0.597 & (0.567, 0.627) & 0.545 & (0.536, 0.554) \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "    \n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. $\\square$: the large dataset size makes training adversarial label DRO models impractical (since per-example gradients must be computed). We leave these experiments to future work. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-income-pubcov}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{Income}} & \\multicolumn{4}{c}{\\textbf{Public Health Ins.}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.829 & (0.827, 0.83) & 0.819 & (0.816, 0.822) & $\\square$ & $\\square$ & $\\square$ & $\\square$ \\\\\n",
    "\tCatBoost & 0.832 & (0.83, 0.834) & 0.814 & (0.811, 0.817) & 0.814 & (0.812, 0.815) & 0.69 & (0.689, 0.691) \\\\\n",
    "\tDRO & 0.828 & (0.826, 0.83) & 0.818 & (0.816, 0.821) & 0.809 & (0.808, 0.81) & 0.647 & (0.646, 0.648) \\\\\n",
    "\tFT-Transformer & 0.825 & (0.823, 0.827) & 0.818 & (0.815, 0.821) & 0.807 & (0.806, 0.808) & 0.662 & (0.661, 0.663) \\\\\n",
    "\tLabel Group DRO & 0.819 & (0.817, 0.821) & 0.818 & (0.815, 0.821) & 0.776 & (0.775, 0.777) & 0.364 & (0.363, 0.365) \\\\\n",
    "\tLightGBM & 0.822 & (0.82, 0.824) & 0.809 & (0.806, 0.812) & 0.803 & (0.802, 0.804) & 0.639 & (0.638, 0.64) \\\\\n",
    "\tMLP & 0.828 & (0.826, 0.829) & 0.813 & (0.81, 0.816) & 0.808 & (0.806, 0.809) & 0.612 & (0.611, 0.613) \\\\\n",
    "\tNODE & 0.831 & (0.829, 0.833) & 0.81 & (0.807, 0.813) & 0.811 & (0.81, 0.812) & 0.662 & (0.661, 0.663) \\\\\n",
    "\tResNet & 0.826 & (0.824, 0.828) & 0.815 & (0.812, 0.818) & 0.81 & (0.809, 0.811) & 0.672 & (0.671, 0.673) \\\\\n",
    "\tSAINT & 0.829 & (0.827, 0.831) & 0.81 & (0.807, 0.812) & 0.811 & (0.81, 0.812) & 0.68 & (0.679, 0.681) \\\\\n",
    "\tTabTransformer & 0.818 & (0.816, 0.82) & 0.801 & (0.798, 0.804) & 0.803 & (0.802, 0.804) & 0.588 & (0.587, 0.589) \\\\\n",
    "\tXGBoost & 0.821 & (0.819, 0.823) & 0.792 & (0.789, 0.795) & 0.805 & (0.804, 0.806) & 0.661 & (0.66, 0.662) \\\\\n",
    "\tCORAL & 0.817 & (0.815, 0.819) & 0.791 & (0.788, 0.793) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tDANN & 0.815 & (0.813, 0.817) & 0.812 & (0.809, 0.815) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tGroup DRO & 0.827 & (0.826, 0.829) & 0.813 & (0.81, 0.815) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tIRM & 0.756 & (0.754, 0.758) & 0.699 & (0.696, 0.702) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMMD & 0.816 & (0.814, 0.818) & 0.768 & (0.765, 0.771) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tMixUp & 0.821 & (0.819, 0.823) & 0.794 & (0.791, 0.797) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\tVREX & 0.714 & (0.712, 0.716) & 0.64 & (0.637, 0.644) & $\\star$ & $\\star$ & $\\star$ & $\\star$ \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. Note that domain generalization models can only be trained on datasets with more than one training subdomain (see Table \\ref{tab:tasks-summary} for domain generalization datasets and Section \\ref{sec:models} for a list of domain generalization models). $\\star$: domain generalization models cannot be trained when only one training subdomain is present. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-sepsis-unemployment}\n",
    "\\hspace*{-0.6in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{Sepsis}} & \\multicolumn{4}{c}{\\textbf{Unemployment}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.972 & (0.971, 0.973) & 0.96 & (0.959, 0.961) \\\\\n",
    "\tCatBoost & 0.988 & (0.987, 0.989) & 0.925 & (0.923, 0.926) & 0.973 & (0.973, 0.974) & 0.962 & (0.961, 0.963) \\\\\n",
    "\tDRO & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.973 & (0.972, 0.973) & 0.961 & (0.96, 0.962) \\\\\n",
    "\tFT-Transformer & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.973 & (0.972, 0.974) & 0.962 & (0.961, 0.962) \\\\\n",
    "\tLabel Group DRO & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.947 & (0.946, 0.948) & 0.926 & (0.925, 0.927) \\\\\n",
    "\tLightGBM & 0.988 & (0.987, 0.989) & 0.928 & (0.926, 0.929) & 0.973 & (0.972, 0.974) & 0.96 & (0.96, 0.961) \\\\\n",
    "\tMLP & 0.988 & (0.987, 0.989) & 0.925 & (0.923, 0.926) & 0.973 & (0.972, 0.973) & 0.96 & (0.959, 0.961) \\\\\n",
    "\tNODE & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.973 & (0.972, 0.974) & 0.962 & (0.961, 0.963) \\\\\n",
    "\tResNet & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.972 & (0.971, 0.972) & 0.959 & (0.958, 0.96) \\\\\n",
    "\tSAINT & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.973 & (0.972, 0.974) & 0.962 & (0.961, 0.963) \\\\\n",
    "\tTabTransformer & 0.988 & (0.987, 0.989) & 0.925 & (0.924, 0.926) & 0.972 & (0.971, 0.973) & 0.961 & (0.96, 0.962) \\\\\n",
    "\tXGBoost & 0.988 & (0.987, 0.989) & 0.925 & (0.923, 0.926) & 0.973 & (0.972, 0.973) & 0.961 & (0.961, 0.962) \\\\\n",
    "\tCORAL & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.964 & (0.963, 0.965) & 0.95 & (0.949, 0.951) \\\\\n",
    "\tDANN & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.966 & (0.965, 0.967) & 0.948 & (0.947, 0.95) \\\\\n",
    "\tGroup DRO & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.971 & (0.97, 0.972) & 0.958 & (0.957, 0.959) \\\\\n",
    "\tIRM & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.966 & (0.965, 0.967) & 0.948 & (0.947, 0.95) \\\\\n",
    "\tMMD & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.966 & (0.966, 0.967) & 0.953 & (0.952, 0.954) \\\\\n",
    "\tMixUp & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.844 & (0.842, 0.846) & 0.776 & (0.774, 0.778) \\\\\n",
    "\tVREX & $\\star$ & $\\star$ & $\\star$ & $\\star$ & 0.873 & (0.871, 0.874) & 0.8 & (0.798, 0.802) \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\begin{table}[!h]\n",
    "\\caption{Best (In-Distribution and Out-Of-Distribution) accuracy pair observed on each benchmark task. See also Figures \\ref{fig:hero-scatter}, \\ref{fig:main-results},\\ref{fig:main-results-2}.}\n",
    "\\label{tab:full-results-voting}\n",
    "\\hspace*{-0.4in}\n",
    "\\rowcolors{2}{gray!12}{white}\n",
    "\\centering\n",
    "\\begin{tabular}{lllll|llll}\n",
    "    \t\\toprule \\textbf{Estimator} & \\multicolumn{4}{c}{\\textbf{Voting}} \\\\ \n",
    "\t & \\multicolumn{2}{c}{\\textbf{ID Acc. (95\\% CI)}} & \\multicolumn{2}{c}{\\textbf{OOD Acc. (95\\% CI)}} \\\\ \\midrule\n",
    "\tAdv. Label DRO & 0.875 & (0.843, 0.902) & 0.852 & (0.839, 0.865) \\\\\n",
    "\tCatBoost & 0.883 & (0.852, 0.909) & 0.855 & (0.842, 0.868) \\\\\n",
    "\tDRO & 0.881 & (0.85, 0.907) & 0.853 & (0.839, 0.866) \\\\\n",
    "\tFT-Transformer & 0.879 & (0.848, 0.906) & 0.855 & (0.841, 0.868) \\\\\n",
    "\tLabel Group DRO & 0.862 & (0.829, 0.89) & 0.839 & (0.825, 0.852) \\\\\n",
    "\tLightGBM & 0.881 & (0.85, 0.907) & 0.855 & (0.841, 0.868) \\\\\n",
    "\tMLP & 0.892 & (0.862, 0.918) & 0.86 & (0.847, 0.873) \\\\\n",
    "\tNODE & 0.885 & (0.854, 0.911) & 0.851 & (0.838, 0.864) \\\\\n",
    "\tResNet & 0.887 & (0.856, 0.912) & 0.836 & (0.822, 0.849) \\\\\n",
    "\tSAINT & 0.888 & (0.858, 0.914) & 0.858 & (0.845, 0.871) \\\\\n",
    "\tTabTransformer & 0.877 & (0.846, 0.904) & 0.859 & (0.845, 0.872) \\\\\n",
    "\tXGBoost & 0.898 & (0.869, 0.923) & 0.851 & (0.838, 0.864) \\\\\n",
    "\tCORAL & 0.883 & (0.852, 0.909) & 0.846 & (0.832, 0.859) \\\\\n",
    "\tDANN & 0.892 & (0.862, 0.918) & 0.852 & (0.838, 0.865) \\\\\n",
    "\tGroup DRO & 0.877 & (0.846, 0.904) & 0.852 & (0.839, 0.865) \\\\\n",
    "\tIRM & 0.804 & (0.767, 0.837) & 0.758 & (0.742, 0.774) \\\\\n",
    "\tMMD & 0.892 & (0.862, 0.918) & 0.849 & (0.835, 0.862) \\\\\n",
    "\tMixUp & 0.892 & (0.862, 0.918) & 0.851 & (0.837, 0.864) \\\\\n",
    "\tVREX & 0.804 & (0.767, 0.837) & 0.754 & (0.737, 0.77) \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
